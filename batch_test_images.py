"""
ë”¸ê¸°ì´ë¯¸ì§€ í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.cv import read_image
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json
from datetime import datetime
import random

def visualize_detections(image: np.ndarray, predictions: list):
    """
    ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        image: RGB ì´ë¯¸ì§€ (SAHI read_image ì¶œë ¥)
        predictions: ê²€ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # RGB -> BGR ë³€í™˜ (OpenCVëŠ” BGR ì‚¬ìš©)
    vis_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    
    # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ (0: OK=ì´ˆë¡, 1: NG=ë¹¨ê°•)
    colors = {
        0: (0, 255, 0),    # OK: ì´ˆë¡
        1: (0, 0, 255),    # NG: ë¹¨ê°•
    }
    
    class_names = {0: "OK", 1: "NG"}
    
    # ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •
    img_area = vis_image.shape[0] * vis_image.shape[1]
    scale_factor = np.sqrt(img_area / (640 * 640))
    thickness = max(1, int(2 * scale_factor))
    font_scale = max(0.5, 0.6 * scale_factor)
    
    for pred in predictions:
        class_id = pred.category.id
        class_name = class_names.get(class_id, str(class_id))
        confidence = pred.score.value
        bbox = pred.bbox.to_voc_bbox()  # [x1, y1, x2, y2]
        
        x1, y1, x2, y2 = map(int, bbox)
        color = colors.get(class_id, (255, 0, 0))
        
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, thickness)
        
        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_name}: {confidence:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
        
        # ë°°ê²½ ë°•ìŠ¤
        cv2.rectangle(vis_image, 
                     (x1, y1 - label_size[1] - 10),
                     (x1 + label_size[0], y1),
                     color, -1)
        
        # í…ìŠ¤íŠ¸
        cv2.putText(vis_image, label,
                   (x1, y1 - 5),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   font_scale,
                   (255, 255, 255),
                   thickness)
        
        # ì¤‘ì‹¬ì 
        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
        cv2.circle(vis_image, (cx, cy), max(3, int(5 * scale_factor)), color, -1)
    
    return vis_image


def batch_test_images(
    model_path: str,
    ok_dir: str,
    ng_dir: str,
    slice_size: int = 640,
    overlap: float = 0.3,
    conf_threshold: float = 0.85,
    num_samples: int = None,
    save_results: bool = True,
    save_images: bool = True
):
    """
    OKì™€ NG í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        ok_dir: OK ì´ë¯¸ì§€ í´ë”
        ng_dir: NG ì´ë¯¸ì§€ í´ë”
        slice_size: ìŠ¬ë¼ì´ìŠ¤ í¬ê¸°
        overlap: ì˜¤ë²„ë© ë¹„ìœ¨
        conf_threshold: Confidence threshold
        num_samples: ê° í´ë˜ìŠ¤ë‹¹ í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        save_results: ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
    """
    print("=" * 70)
    print("ë°°ì¹˜ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"ğŸ¯ Confidence threshold: {conf_threshold}")
    print(f"ğŸ“¦ Slice size: {slice_size}x{slice_size}")
    print(f"ğŸ”„ Overlap: {overlap*100:.0f}%")
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë“œ: {model_path}")
    detection_model = AutoDetectionModel.from_pretrained(
        model_type="yolov8",
        model_path=model_path,
        confidence_threshold=conf_threshold,
        device="cpu"
    )
    
    # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ (ëœë¤ ìƒ˜í”Œë§)
    all_ok_images = list(Path(ok_dir).glob("*.jpg"))
    all_ng_images = list(Path(ng_dir).glob("*.jpg"))
    
    if num_samples is not None:
        ok_images = random.sample(all_ok_images, min(num_samples, len(all_ok_images)))
        ng_images = random.sample(all_ng_images, min(num_samples, len(all_ng_images)))
    else:
        ok_images = all_ok_images
        ng_images = all_ng_images
    
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€:")
    print(f"   âœ… OK: {len(ok_images)}ê°œ")
    print(f"   âŒ NG: {len(ng_images)}ê°œ")
    print(f"   ì´: {len(ok_images) + len(ng_images)}ê°œ")
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = {
        'ok_images': [],
        'ng_images': [],
        'summary': {}
    }
    
    # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if save_images:
        output_dir = Path("batch_test_results")
        output_dir.mkdir(exist_ok=True)
        (output_dir / "ok_correct").mkdir(exist_ok=True)
        (output_dir / "ok_wrong").mkdir(exist_ok=True)
        (output_dir / "ng_correct").mkdir(exist_ok=True)
        (output_dir / "ng_wrong").mkdir(exist_ok=True)
        print(f"\nğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬: {output_dir}/")
    
    # OK ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 70)
    print("âœ… OK ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë¼ë²¨: OK)")
    print("=" * 70)
    
    ok_correct = 0
    ok_wrong = 0
    ok_details = []
    
    for img_path in tqdm(ok_images, desc="OK í…ŒìŠ¤íŠ¸"):
        try:
            image = read_image(str(img_path))
            
            result = get_sliced_prediction(
                image,
                detection_model,
                slice_height=slice_size,
                slice_width=slice_size,
                overlap_height_ratio=overlap,
                overlap_width_ratio=overlap,
                postprocess_type="NMS",
                postprocess_match_metric="IOS",
                postprocess_match_threshold=0.5,
                postprocess_class_agnostic=False,
                verbose=0
            )
            
            # ê²°ê³¼ ë¶„ì„
            ok_count = sum(1 for pred in result.object_prediction_list if pred.category.id == 0)
            ng_count = sum(1 for pred in result.object_prediction_list if pred.category.id == 1)
            
            # ì •í™•ë„ ê³„ì‚° (OKê°€ ë” ë§ìœ¼ë©´ ì •ë‹µ)
            is_correct = ok_count >= ng_count if len(result.object_prediction_list) > 0 else True
            
            if is_correct:
                ok_correct += 1
            else:
                ok_wrong += 1
            
            detail = {
                'image': str(img_path.name),
                'total_detections': len(result.object_prediction_list),
                'ok_count': ok_count,
                'ng_count': ng_count,
                'correct': is_correct
            }
            ok_details.append(detail)
            results['ok_images'].append(detail)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
            if save_images:
                vis_image = visualize_detections(image, result.object_prediction_list)
                subdir = "ok_correct" if is_correct else "ok_wrong"
                save_path = output_dir / subdir / f"{img_path.stem}_result.jpg"
                cv2.imwrite(str(save_path), vis_image)
            
        except Exception as e:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ({img_path.name}): {e}")
    
    # NG ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 70)
    print("âŒ NG ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë¼ë²¨: NG)")
    print("=" * 70)
    
    ng_correct = 0
    ng_wrong = 0
    ng_details = []
    
    for img_path in tqdm(ng_images, desc="NG í…ŒìŠ¤íŠ¸"):
        try:
            image = read_image(str(img_path))
            
            result = get_sliced_prediction(
                image,
                detection_model,
                slice_height=slice_size,
                slice_width=slice_size,
                overlap_height_ratio=overlap,
                overlap_width_ratio=overlap,
                postprocess_type="NMS",
                postprocess_match_metric="IOS",
                postprocess_match_threshold=0.5,
                postprocess_class_agnostic=False,
                verbose=0
            )
            
            # ê²°ê³¼ ë¶„ì„
            ok_count = sum(1 for pred in result.object_prediction_list if pred.category.id == 0)
            ng_count = sum(1 for pred in result.object_prediction_list if pred.category.id == 1)
            
            # ì •í™•ë„ ê³„ì‚° (NGê°€ ë” ë§ìœ¼ë©´ ì •ë‹µ)
            is_correct = ng_count > ok_count if len(result.object_prediction_list) > 0 else False
            
            if is_correct:
                ng_correct += 1
            else:
                ng_wrong += 1
            
            detail = {
                'image': str(img_path.name),
                'total_detections': len(result.object_prediction_list),
                'ok_count': ok_count,
                'ng_count': ng_count,
                'correct': is_correct
            }
            ng_details.append(detail)
            results['ng_images'].append(detail)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
            if save_images:
                vis_image = visualize_detections(image, result.object_prediction_list)
                subdir = "ng_correct" if is_correct else "ng_wrong"
                save_path = output_dir / subdir / f"{img_path.stem}_result.jpg"
                cv2.imwrite(str(save_path), vis_image)
            
        except Exception as e:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ({img_path.name}): {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    total_images = len(ok_images) + len(ng_images)
    total_correct = ok_correct + ng_correct
    total_accuracy = (total_correct / total_images * 100) if total_images > 0 else 0
    
    ok_accuracy = (ok_correct / len(ok_images) * 100) if len(ok_images) > 0 else 0
    ng_accuracy = (ng_correct / len(ng_images) * 100) if len(ng_images) > 0 else 0
    
    print(f"\nâœ… OK ì´ë¯¸ì§€ (ì‹¤ì œ: OK):")
    print(f"   ì •ë‹µ: {ok_correct}/{len(ok_images)}ê°œ ({ok_accuracy:.1f}%)")
    print(f"   ì˜¤ë‹µ: {ok_wrong}ê°œ")
    
    print(f"\nâŒ NG ì´ë¯¸ì§€ (ì‹¤ì œ: NG):")
    print(f"   ì •ë‹µ: {ng_correct}/{len(ng_images)}ê°œ ({ng_accuracy:.1f}%)")
    print(f"   ì˜¤ë‹µ: {ng_wrong}ê°œ")
    
    print(f"\nğŸ¯ ì „ì²´ ì •í™•ë„: {total_correct}/{total_images}ê°œ ({total_accuracy:.1f}%)")
    
    # Confusion Matrix
    print(f"\nğŸ“Š Confusion Matrix:")
    print(f"                ì˜ˆì¸¡ OK    ì˜ˆì¸¡ NG")
    print(f"   ì‹¤ì œ OK:     {ok_correct:4d}       {ok_wrong:4d}")
    print(f"   ì‹¤ì œ NG:     {ng_wrong:4d}       {ng_correct:4d}")
    
    # ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ì¶œë ¥
    if ok_wrong > 0:
        print(f"\nâš ï¸ OKë¥¼ NGë¡œ ì˜¤ë¶„ë¥˜í•œ ì‚¬ë¡€:")
        wrong_ok = [d for d in ok_details if not d['correct']]
        for i, detail in enumerate(wrong_ok[:5], 1):
            print(f"   [{i}] {detail['image']}: OK {detail['ok_count']}ê°œ, NG {detail['ng_count']}ê°œ")
    
    if ng_wrong > 0:
        print(f"\nâš ï¸ NGë¥¼ OKë¡œ ì˜¤ë¶„ë¥˜í•œ ì‚¬ë¡€:")
        wrong_ng = [d for d in ng_details if not d['correct']]
        for i, detail in enumerate(wrong_ng[:5], 1):
            print(f"   [{i}] {detail['image']}: OK {detail['ok_count']}ê°œ, NG {detail['ng_count']}ê°œ")
    
    # ê²°ê³¼ ì €ì¥
    if save_results:
        results['summary'] = {
            'timestamp': datetime.now().isoformat(),
            'model_path': model_path,
            'conf_threshold': conf_threshold,
            'slice_size': slice_size,
            'overlap': overlap,
            'total_images': total_images,
            'ok_images': len(ok_images),
            'ng_images': len(ng_images),
            'ok_correct': ok_correct,
            'ok_wrong': ok_wrong,
            'ok_accuracy': ok_accuracy,
            'ng_correct': ng_correct,
            'ng_wrong': ng_wrong,
            'ng_accuracy': ng_accuracy,
            'total_correct': total_correct,
            'total_accuracy': total_accuracy
        }
        
        output_file = f"batch_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    
    if save_images:
        print(f"\nğŸ–¼ï¸ ê²°ê³¼ ì´ë¯¸ì§€:")
        print(f"   âœ… OK ì •ë‹µ: batch_test_results/ok_correct/ ({ok_correct}ê°œ)")
        print(f"   âŒ OK ì˜¤ë‹µ: batch_test_results/ok_wrong/ ({ok_wrong}ê°œ)")
        print(f"   âœ… NG ì •ë‹µ: batch_test_results/ng_correct/ ({ng_correct}ê°œ)")
        print(f"   âŒ NG ì˜¤ë‹µ: batch_test_results/ng_wrong/ ({ng_wrong}ê°œ)")
    
    print("=" * 70)
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--model", type=str,
                       default="runs/detect/strawberry_ok_ng/weights/best.pt",
                       help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--ok-dir", type=str,
                       default="ë”¸ê¸°ì´ë¯¸ì§€/ì •ìƒ",
                       help="OK ì´ë¯¸ì§€ í´ë”")
    parser.add_argument("--ng-dir", type=str,
                       default="ë”¸ê¸°ì´ë¯¸ì§€/NG",
                       help="NG ì´ë¯¸ì§€ í´ë”")
    parser.add_argument("--slice-size", type=int, default=640,
                       help="ìŠ¬ë¼ì´ìŠ¤ í¬ê¸°")
    parser.add_argument("--overlap", type=float, default=0.3,
                       help="ì˜¤ë²„ë© ë¹„ìœ¨")
    parser.add_argument("--conf", type=float, default=0.85,
                       help="Confidence threshold")
    parser.add_argument("--num-samples", type=int, default=None,
                       help="ê° í´ë˜ìŠ¤ë‹¹ í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)")
    parser.add_argument("--save-images", action="store_true", default=True,
                       help="ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€")
    
    args = parser.parse_args()
    
    results = batch_test_images(
        model_path=args.model,
        ok_dir=args.ok_dir,
        ng_dir=args.ng_dir,
        slice_size=args.slice_size,
        overlap=args.overlap,
        conf_threshold=args.conf,
        num_samples=args.num_samples,
        save_results=True,
        save_images=args.save_images
    )

